{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.6 64-bit ('DS_hw1_2': conda)",
   "display_name": "Python 3.6.6 64-bit ('DS_hw1_2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5cb71a230f220b52020845f1a55222cad2335f7f5ed4ac5dcabdfc6cd4959c55"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import notebook\n",
    "def tqdm(x, **kargs):\n",
    "    return notebook.tqdm(x, leave=False, **kargs)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, datatype, transform):\n",
    "        ##############################################\n",
    "        ### Initialize paths, transforms, and so on\n",
    "        ##############################################\n",
    "        self.transform = transform\n",
    "\n",
    "        filename = open('./crawling/{}_list'.format(datatype), 'r')\n",
    "        info = filename.readlines()\n",
    "        self.images = [row.split(',')[0] for row in info]\n",
    "        self.labels = [int(row.split(',')[1].strip()) for row in info]\n",
    "        \n",
    "        assert len(self.images) == len(self.labels), 'mismatched length!'\n",
    "        print(\"image shape: {}, label shape: {}\".format(len(self.images), len(self.labels)))\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ##############################################\n",
    "        # 1. Read from file (using numpy.fromfile, PIL.Image.open)\n",
    "        # 2. Preprocess the data (torchvision.Transform).\n",
    "        # 3. Return the data (e.g. image and label)\n",
    "        ##############################################\n",
    "        \n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        ##############################################\n",
    "        ### Indicate the total size of the dataset\n",
    "        ##############################################\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 8\n",
    "# min (100, 100), max (5792, 8688)\n",
    "resize_size = (224, 224)\n",
    "MODEL = 'efficientnet-b4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image shape: 45242, label shape: 45242\n"
     ]
    }
   ],
   "source": [
    "trainset = customDataset(datatype='train',\n",
    "                         transform=data_transforms['train'])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image shape: 11309, label shape: 11309\n"
     ]
    }
   ],
   "source": [
    "testset = customDataset(datatype='test',\n",
    "                        transform=data_transforms['test'])\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of image: torch.Size([32, 3, 224, 224])\nType of image: torch.float32\nSize of label: torch.Size([32])\nType of label: torch.int64\n"
     ]
    }
   ],
   "source": [
    "for imgs, lbls in trainloader:\n",
    "    print('Size of image:', imgs.size())  \n",
    "    print('Type of image:', imgs.dtype)\n",
    "    print('Size of label:', lbls.size())  \n",
    "    print('Type of label:', lbls.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (17): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (18): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (19): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (20): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (21): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (22): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (23): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (24): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (25): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (26): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (27): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (28): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (29): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (30): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (31): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.4, inplace=False)\n",
       "    (_fc): Linear(in_features=1792, out_features=1000, bias=True)\n",
       "    (_swish): MemoryEfficientSwish()\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=1000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        # self.pretrained = models.resnet50(pretrained=True)\n",
    "        # self.pretrained = models.densenet161(pretrained=True)\n",
    "        self.pretrained = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "\n",
    "        # freeze pretrained models\n",
    "        # ct = 0\n",
    "        for child in self.pretrained.children():\n",
    "            # ct += 1\n",
    "            # if ct < 5:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(1000, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive: 15433, negative: 32895\n",
    "class_weights = torch.FloatTensor([1, 2.6646]).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "epochs = 5\n",
    "size = 45242 // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Epochs: '), FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fe3bb03aa47443592c1f58f7bbc7a0c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================== EPOCH INFO: 1 ==================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Progress:'), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "784569fa05104118b6a193326f3b0186"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================================\n",
      "Batch acc = 0.3125 (10.0 / 32)\n",
      "Batch f1-score = 0.26666666666666666\n",
      "(0.16666666666666666, 0.6666666666666666, 0.26666666666666666, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([ 8, 24])) (array([0, 1]), array([26,  6]))\n",
      "[1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1] [0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "[1,   501] loss: 0.738\n",
      "==============================================\n",
      "Batch acc = 0.6875 (22.0 / 32)\n",
      "Batch f1-score = 0.0\n",
      "(0.0, 0.0, 0.0, None)\n",
      "Prediction distribution:\n",
      "(array([0]), array([32])) (array([0, 1]), array([22, 10]))\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      "[1,  1001] loss: 0.734\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31c8b04b56084348b718bd2840e6d890"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining prediction:\n(array([0., 1.]), array([26043, 19199])) (array([0., 1.]), array([32895, 12347]))\n[1. 1. 0. 1. 0. 1. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\nF1 of model on train set:  0.3637228174728967\n(0.298817646752435, 0.46464728274074674, 0.3637228174728967, None)\n(0.5225033209379424, 0.5277028783364777, 0.5115807748499914, None)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=354.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f2b9917497546b083b48ff9b873fa36"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing prediction:\n",
      "(array([0., 1.]), array([5929, 5380])) (array([0., 1.]), array([8223, 3086]))\n",
      "[1. 1. 1. 1. 0. 0. 1. 1. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "F1 of my model on test set:  0.3649893692416726\n",
      "(0.2871747211895911, 0.5006480881399871, 0.3649893692416726, None)\n",
      "(0.5136328994715033, 0.5171366428782144, 0.49255686664457854, None)\n",
      "================== EPOCH INFO: 2 ==================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Progress:'), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da741e3467c94381a6cb9f33c9474197"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================================\n",
      "Batch acc = 0.46875 (15.0 / 32)\n",
      "Batch f1-score = 0.45161290322580644\n",
      "(0.4117647058823529, 0.5, 0.45161290322580644, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([15, 17])) (array([0, 1]), array([18, 14]))\n",
      "[1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1] [0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1]\n",
      "[2,   501] loss: 0.733\n",
      "==============================================\n",
      "Batch acc = 0.4375 (14.0 / 32)\n",
      "Batch f1-score = 0.3076923076923077\n",
      "(0.21052631578947367, 0.5714285714285714, 0.3076923076923077, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([13, 19])) (array([0, 1]), array([25,  7]))\n",
      "[0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0] [0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0]\n",
      "[2,  1001] loss: 0.734\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "917ff8843c3f4bae942bbf51a328a414"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining prediction:\n(array([0., 1.]), array([21560, 23682])) (array([0., 1.]), array([32895, 12347]))\n[0. 1. 0. 1. 0. 1. 1. 0. 0. 1.] [1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\nF1 of model on train set:  0.3858558383524383\n(0.29351406131238916, 0.5629707621284522, 0.3858558383524383, None)\n(0.5216178840884765, 0.5271762155375503, 0.4897601659855112, None)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=354.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55fd13be06144c47a18437f51a3d7076"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing prediction:\n",
      "(array([0., 1.]), array([5250, 6059])) (array([0., 1.]), array([8223, 3086]))\n",
      "[1. 1. 1. 1. 0. 1. 0. 0. 1. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "F1 of my model on test set:  0.38512848551120826\n",
      "(0.29064202013533585, 0.5706416072585871, 0.38512848551120826, None)\n",
      "(0.5191305338771918, 0.5239806601293544, 0.4838876302713764, None)\n",
      "================== EPOCH INFO: 3 ==================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Progress:'), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f25b485a0c14af387dc09378a4810c7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================================\n",
      "Batch acc = 0.5625 (18.0 / 32)\n",
      "Batch f1-score = 0.5\n",
      "(0.3684210526315789, 0.7777777777777778, 0.5, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([13, 19])) (array([0, 1]), array([23,  9]))\n",
      "[0 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1] [0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[3,   501] loss: 0.729\n",
      "==============================================\n",
      "Batch acc = 0.21875 (7.0 / 32)\n",
      "Batch f1-score = 0.2424242424242424\n",
      "(0.13793103448275862, 1.0, 0.2424242424242424, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([ 3, 29])) (array([0, 1]), array([28,  4]))\n",
      "[1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1] [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0]\n",
      "[3,  1001] loss: 0.730\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "915fe318489047c6bb9445c615e23f4d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining prediction:\n(array([0., 1.]), array([44007,  1235])) (array([0., 1.]), array([32895, 12347]))\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\nF1 of model on train set:  0.05713444264467677\n(0.31417004048582997, 0.03142463756378067, 0.05713444264467677, None)\n(0.5212089096241498, 0.5028380217762664, 0.44530540758537446, None)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=354.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fc51a9837fd4f039a96c7598eedea60"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing prediction:\n",
      "(array([0., 1.]), array([11140,   169])) (array([0., 1.]), array([8223, 3086]))\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "F1 of my model on test set:  0.027649769585253454\n",
      "(0.26627218934911245, 0.01458198314970836, 0.027649769585253454, None)\n",
      "(0.4966459690013067, 0.4997511642612217, 0.4320968467819879, None)\n",
      "================== EPOCH INFO: 4 ==================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Progress:'), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2caa2faf0164a12a0578eed6ac6f8b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================================\n",
      "Batch acc = 0.4375 (14.0 / 32)\n",
      "Batch f1-score = 0.4\n",
      "(0.2608695652173913, 0.8571428571428571, 0.4, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([ 9, 23])) (array([0, 1]), array([25,  7]))\n",
      "[1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1] [0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      "[4,   501] loss: 0.732\n",
      "==============================================\n",
      "Batch acc = 0.53125 (17.0 / 32)\n",
      "Batch f1-score = 0.2105263157894737\n",
      "(0.16666666666666666, 0.2857142857142857, 0.2105263157894737, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([20, 12])) (array([0, 1]), array([25,  7]))\n",
      "[0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1] [0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "[4,  1001] loss: 0.730\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f56a85aa4e24d838e3fb3776080da4a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining prediction:\n(array([0., 1.]), array([23769, 21473])) (array([0., 1.]), array([32895, 12347]))\n[1. 0. 0. 0. 0. 0. 0. 0. 1. 1.] [0. 0. 0. 1. 1. 1. 0. 0. 0. 1.]\nF1 of model on train set:  0.3744529863985807\n(0.294881944767848, 0.5128371264274723, 0.3744529863985807, None)\n(0.5209106177202865, 0.5262772043446071, 0.5005471200523188, None)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=354.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8741f4225b84551ba3aabb6c1103e5d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing prediction:\n",
      "(array([0., 1.]), array([6589, 4720])) (array([0., 1.]), array([8223, 3086]))\n",
      "[1. 1. 0. 0. 0. 0. 1. 1. 1. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "F1 of my model on test set:  0.345119139123751\n",
      "(0.2853813559322034, 0.4364873622812703, 0.345119139123751, None)\n",
      "(0.5107283164544913, 0.513148217198035, 0.4999967826323588, None)\n",
      "================== EPOCH INFO: 5 ==================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Progress:'), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99c8d04f489a4d5cbd783871a90ee9a2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================================\n",
      "Batch acc = 0.46875 (15.0 / 32)\n",
      "Batch f1-score = 0.37037037037037035\n",
      "(0.2631578947368421, 0.625, 0.37037037037037035, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([13, 19])) (array([0, 1]), array([24,  8]))\n",
      "[1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1] [0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0]\n",
      "[5,   501] loss: 0.731\n",
      "==============================================\n",
      "Batch acc = 0.34375 (11.0 / 32)\n",
      "Batch f1-score = 0.43243243243243246\n",
      "(0.3076923076923077, 0.7272727272727273, 0.43243243243243246, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([ 6, 26])) (array([0, 1]), array([21, 11]))\n",
      "[1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1] [0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0]\n",
      "[5,  1001] loss: 0.731\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adf1f8e4ac2e450ba463eb5d58af8f31"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining prediction:\n(array([0., 1.]), array([29791, 15451])) (array([0., 1.]), array([32895, 12347]))\n[1. 1. 1. 1. 1. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\nF1 of model on train set:  0.33326138571120223\n(0.2997864215908356, 0.3751518587511136, 0.33326138571120223, None)\n(0.5204077957371787, 0.5231284449554322, 0.518798641041799, None)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=354.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a77983d55ccd4c8fad867a21da897261"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing prediction:\n(array([0., 1.]), array([7522, 3787])) (array([0., 1.]), array([8223, 3086]))\n[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\nF1 of my model on test set:  0.318929143023425\n(0.28941114338526536, 0.35515230071289694, 0.318929143023425, None)\n(0.5124269223972325, 0.5139497366388271, 0.5108142063164124, None)\n"
     ]
    }
   ],
   "source": [
    "### First\n",
    "LR = 1e-2\n",
    "# optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc='Epochs: '):  # loop over the dataset multiple times\n",
    "    print(\"================== EPOCH INFO: {} ==================\".format(epoch + 1))\n",
    "    net.train()     # Train mode\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(trainloader, desc='Progress:'), 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # print(data[0].shape, data[1].shape)\n",
    "        # print(inputs.shape)\n",
    "        # print(labels.shape, outputs.shape)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 0 and i != 0:    # print every size mini-batches\n",
    "            _, y_pred_tag = torch.max(outputs, 1)\n",
    "            correct_results_sum = (y_pred_tag == labels).sum().float()\n",
    "            acc = correct_results_sum / labels.shape[0]\n",
    "\n",
    "            y_pred_np = y_pred_tag.cpu().detach().numpy()\n",
    "            y_true_np = labels.cpu().detach().numpy()\n",
    "            print(\"==============================================\")\n",
    "            print(\"Batch acc = {} ({} / {})\".format(acc, correct_results_sum, labels.shape[0]))\n",
    "            print(\"Batch f1-score = {}\".format(f1_score(y_true_np, y_pred_np, average='binary')))\n",
    "            print(precision_recall_fscore_support(y_true_np, y_pred_np, average='binary'))\n",
    "\n",
    "            print(\"Prediction distribution:\")\n",
    "            print(np.unique(y_pred_np, return_counts=True), np.unique(y_true_np, return_counts=True))\n",
    "            # print(y_pred_np, y_true_np)\n",
    "\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 500))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Eval mode\n",
    "    net.eval()\n",
    "    # Training score\n",
    "    y_pred_train = np.array([])\n",
    "    y_true_train = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for data_test in tqdm(trainloader):\n",
    "            images, labels = data_test[0].to(device), data_test[1].to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, y_pred_tag = torch.max(outputs, 1)\n",
    "            y_pred_train = np.hstack([y_pred_train, y_pred_tag.cpu().detach().numpy()])\n",
    "\n",
    "            y_true_train = np.hstack([y_true_train, labels.cpu().detach().numpy()])\n",
    "\n",
    "    precision_train, recall_train, f1_train, support_train = precision_recall_fscore_support(y_true_train, y_pred_train, average='binary')\n",
    "\n",
    "    print()\n",
    "    print(\"Training prediction:\")\n",
    "    print(np.unique(y_pred_train, return_counts=True), np.unique(y_true_train, return_counts=True))\n",
    "    print(y_pred_train[0:10], y_true_train[0:10])\n",
    "    print(\"F1 of model on train set: \", f1_score(y_true_train, y_pred_train, average='binary'))\n",
    "    print(\"Precision: {}, Recall: {}, F1: {}\".format(precision_train, recall_train, f1_train))\n",
    "\n",
    "    # Testing score\n",
    "    y_pred_test = np.array([])\n",
    "    y_true_test = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for data_test in tqdm(testloader):\n",
    "            images, labels = data_test[0].to(device), data_test[1].to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, y_pred_tag = torch.max(outputs, 1)\n",
    "            y_pred_test = np.hstack([y_pred_test, y_pred_tag.cpu().detach().numpy()])\n",
    "                \n",
    "            y_true_test = np.hstack([y_true_test, labels.cpu().detach().numpy()])\n",
    "    \n",
    "    precision_test, recall_test, f1_test, support_test = precision_recall_fscore_support(y_true_test, y_pred_test, average='binary')\n",
    "\n",
    "    print(\"Testing prediction:\")\n",
    "    print(np.unique(y_pred_test, return_counts=True), np.unique(y_true_test, return_counts=True))\n",
    "    print(y_pred_test[0:10], y_true_test[0:10])\n",
    "    print(\"F1 of my model on test set: \", f1_score(y_true_test, y_pred_test, average='binary'))\n",
    "    print(\"Precision: {}, Recall: {}, F1: {}\".format(precision_test, recall_test, f1_test))\n",
    "\n",
    "\n",
    "    # record precision, recall, f1-score into file\n",
    "    with open('record/train_loss', 'a') as f_train, open('record/test_loss', 'a') as f_test:\n",
    "        f_train.write(str(precision_train) + ', ' + str(recall_train) + ', ' + str(f1_train) + '\\n')\n",
    "        f_test.write(str(precision_test) + ', ' + str(recall_test) + ', ' + str(f1_test) + '\\n')\n",
    "\n",
    "    # save model for every epoch\n",
    "    torch.save(net.state_dict(), './model/{}_{}_{}_{}_{}'.format(MODEL, LR, epoch, f1_train, f1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Epochs: '), FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88719771f0ed49ee8d21663334139560"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================== EPOCH INFO: 1 ==================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Progress:'), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cc4a205b8e64288925b9c6887c708ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================================\n",
      "Batch acc = 0.6875 (22.0 / 32)\n",
      "Batch f1-score = 0.4444444444444445\n",
      "(0.4, 0.5, 0.4444444444444445, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([22, 10])) (array([0, 1]), array([24,  8]))\n",
      "[1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0] [0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "[1,   501] loss: 0.708\n",
      "==============================================\n",
      "Batch acc = 0.46875 (15.0 / 32)\n",
      "Batch f1-score = 0.32\n",
      "(0.26666666666666666, 0.4, 0.32, None)\n",
      "Prediction distribution:\n",
      "(array([0, 1]), array([17, 15])) (array([0, 1]), array([22, 10]))\n",
      "[0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0] [0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0]\n",
      "[1,  1001] loss: 0.700\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2373699005740ffb574d449d8cc659f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining prediction:\n(array([0., 1.]), array([28819, 16423])) (array([0., 1.]), array([32895, 12347]))\n[1. 1. 1. 0. 0. 1. 0. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 0. 1. 0. 1.]\nF1 of model on train set:  0.3598887730274592\n(0.31522864275710893, 0.41929213574147567, 0.3598887730274592, None)\n(0.5332172222425677, 0.5387082353733978, 0.5307399920489404, None)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=354.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d554cf1b578c40e887edce2634373e4b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing prediction:\n",
      "(array([0., 1.]), array([7559, 3750])) (array([0., 1.]), array([8223, 3086]))\n",
      "[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "F1 of my model on test set:  0.3294324166179052\n",
      "(0.3002666666666667, 0.3648736228127025, 0.3294324166179052, None)\n",
      "(0.5204865546589055, 0.5228843366404508, 0.5194874667045931, None)\n",
      "================== EPOCH INFO: 2 ==================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Progress:'), FloatProgress(value=0.0, max=1414.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d1f575b8b4c42b2b4c9c50ebf2e5fcb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================================\nBatch acc = 0.65625 (21.0 / 32)\nBatch f1-score = 0.56\n(0.4666666666666667, 0.7, 0.56, None)\nPrediction distribution:\n(array([0, 1]), array([17, 15])) (array([0, 1]), array([22, 10]))\n[0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0] [0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0]\n[2,   501] loss: 0.698\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d256ab0f4782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# labels = labels.type_as(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DS_hw1_2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-3c3452289f67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DS_hw1_2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DS_hw1_2/lib/python3.6/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \"\"\"\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_avg_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DS_hw1_2/lib/python3.6/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# scale drop connect_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# Head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DS_hw1_2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DS_hw1_2/lib/python3.6/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_ratio\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DS_hw1_2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DS_hw1_2/lib/python3.6/site-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Second\n",
    "LR = 1e-3\n",
    "# optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc='Epochs: '):  # loop over the dataset multiple times\n",
    "    print(\"================== EPOCH INFO: {} ==================\".format(epoch + 1))\n",
    "    net.train()     # Train mode\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(trainloader, desc='Progress:'), 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        # labels = labels.type_as(outputs)\n",
    "\n",
    "        # print(data[0].shape, data[1].shape)\n",
    "        # print(inputs.shape)\n",
    "        # print(labels.shape, outputs.shape)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 0 and i != 0:    # print every size mini-batches\n",
    "            # y_pred_tag = torch.round(torch.sigmoid(outputs))\n",
    "            # correct_results_sum = (y_pred_tag == labels).sum().float()\n",
    "            # acc = correct_results_sum / labels.shape[0]\n",
    "            _, y_pred_tag = torch.max(outputs, 1)\n",
    "            correct_results_sum = (y_pred_tag == labels).sum().float()\n",
    "            acc = correct_results_sum / labels.shape[0]\n",
    "\n",
    "            y_pred_np = y_pred_tag.cpu().detach().numpy()\n",
    "            y_true_np = labels.cpu().detach().numpy()\n",
    "            print(\"==============================================\")\n",
    "            print(\"Batch acc = {} ({} / {})\".format(acc, correct_results_sum, labels.shape[0]))\n",
    "            print(\"Batch f1-score = {}\".format(f1_score(y_true_np, y_pred_np, average='binary')))\n",
    "            print(precision_recall_fscore_support(y_true_np, y_pred_np, average='binary'))\n",
    "\n",
    "            print(\"Prediction distribution:\")\n",
    "            print(np.unique(y_pred_np, return_counts=True), np.unique(y_true_np, return_counts=True))\n",
    "            print(y_pred_np, y_true_np)\n",
    "            # print(outputs)\n",
    "\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 500))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Eval mode\n",
    "    net.eval()\n",
    "    # Training score\n",
    "    y_pred_train = np.array([])\n",
    "    y_true_train = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for data_test in tqdm(trainloader):\n",
    "            images, labels = data_test[0].to(device), data_test[1].to(device)\n",
    "            outputs = net(images)\n",
    "            # outputs = torch.round(torch.sigmoid(outputs))\n",
    "            _, y_pred_tag = torch.max(outputs, 1)\n",
    "            y_pred_train = np.hstack([y_pred_train, y_pred_tag.cpu().detach().numpy()])\n",
    "\n",
    "            # for pred_class in outputs.cpu().detach().numpy():\n",
    "            #     y_pred_train = np.hstack([y_pred_train, pred_class])\n",
    "\n",
    "            y_true_train = np.hstack([y_true_train, labels.cpu().detach().numpy()])\n",
    "\n",
    "    print()\n",
    "    print(\"Training prediction:\")\n",
    "    print(np.unique(y_pred_train, return_counts=True), np.unique(y_true_train, return_counts=True))\n",
    "    print(y_pred_train[0:10], y_true_train[0:10])\n",
    "    print(\"F1 of model on train set: \", f1_score(y_true_train, y_pred_train))\n",
    "    print(precision_recall_fscore_support(y_true_train, y_pred_train, average='binary'))\n",
    "\n",
    "    # Testing score\n",
    "    y_pred_test = np.array([])\n",
    "    y_true_test = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for data_test in tqdm(testloader):\n",
    "            images, labels = data_test[0].to(device), data_test[1].to(device)\n",
    "            outputs = net(images)\n",
    "            # outputs = torch.round(torch.sigmoid(outputs))\n",
    "\n",
    "            _, y_pred_tag = torch.max(outputs, 1)\n",
    "            y_pred_test = np.hstack([y_pred_test, y_pred_tag.cpu().detach().numpy()])\n",
    "\n",
    "            # for pred_class in outputs.cpu().detach().numpy():\n",
    "            #     y_pred_test = np.hstack([y_pred_test, pred_class])\n",
    "                \n",
    "            y_true_test = np.hstack([y_true_test, labels.cpu().detach().numpy()])\n",
    "    \n",
    "    print(\"Testing prediction:\")\n",
    "    print(np.unique(y_pred_test, return_counts=True), np.unique(y_true_test, return_counts=True))\n",
    "    print(y_pred_test[0:10], y_true_test[0:10])\n",
    "    print(\"F1 of my model on test set: \", f1_score(y_true_test, y_pred_test))\n",
    "    print(precision_recall_fscore_support(y_true_test, y_pred_test, average='binary'))\n",
    "\n",
    "    precision_train, recall_train, f1_train, support_train = precision_recall_fscore_support(y_true_train, y_pred_train, average='binary')\n",
    "    torch.save(net.state_dict(), './model/{}_{}_{}_{}'.format(MODEL, LR, epoch, f1_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval mode\n",
    "net.eval()\n",
    "# Training score\n",
    "y_pred_train = np.array([])\n",
    "y_true_train = np.array([])\n",
    "with torch.no_grad():\n",
    "    for data_test in tqdm(trainloader):\n",
    "        images, labels = data_test[0].to(device), data_test[1].to(device)\n",
    "        outputs = net(images)\n",
    "        # outputs = torch.round(torch.sigmoid(outputs))\n",
    "        _, y_pred_tag = torch.max(outputs, 1)\n",
    "        y_pred_train = np.hstack([y_pred_train, y_pred_tag.cpu().detach().numpy()])\n",
    "\n",
    "        # for pred_class in outputs.cpu().detach().numpy():\n",
    "        #     y_pred_train = np.hstack([y_pred_train, pred_class])\n",
    "\n",
    "        y_true_train = np.hstack([y_true_train, labels.cpu().detach().numpy()])\n",
    "\n",
    "precision_train, recall_train, f1_train, support_train = precision_recall_fscore_support(y_true_train, y_pred_train, average='binary')\n",
    "print()\n",
    "print(\"Training prediction:\")\n",
    "print(np.unique(y_pred_train, return_counts=True), np.unique(y_true_train, return_counts=True))\n",
    "print(y_pred_train[0:10], y_true_train[0:10])\n",
    "print(\"F1 of model on train set: \", f1_score(y_true_train, y_pred_train))\n",
    "print(\"Precision: {}, Recall: {}, F1: {}\".format(precision_train, recall_train, f1_train))\n",
    "np.unique(y_pred_np, return_counts=True), np.unique(y_true_np, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=354.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "225322f969664a568c000d1479cd155a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of my model on test set:  0.40753180661577604\nPrecision: 0.2970767176138893, Recall: 0.6487362281270252, F1: 0.40753180661577604\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((array([0., 1.]), array([4570, 6739])),\n",
       " (array([0., 1.]), array([8223, 3086])))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Testing score\n",
    "y_pred_test = np.array([])\n",
    "y_true_test = np.array([])\n",
    "with torch.no_grad():\n",
    "    for data_test in tqdm(testloader):\n",
    "        images, labels = data_test[0].to(device), data_test[1].to(device)\n",
    "        outputs = net(images)\n",
    "        # outputs = torch.round(torch.sigmoid(outputs))\n",
    "\n",
    "        _, y_pred_tag = torch.max(outputs, 1)\n",
    "        y_pred_test = np.hstack([y_pred_test, y_pred_tag.cpu().detach().numpy()])\n",
    "\n",
    "        # for pred_class in outputs.cpu().detach().numpy():\n",
    "        #     y_pred_test = np.hstack([y_pred_test, pred_class])\n",
    "            \n",
    "        y_true_test = np.hstack([y_true_test, labels.cpu().detach().numpy()])\n",
    "precision_test, recall_test, f1_test, support_test = precision_recall_fscore_support(y_true_test, y_pred_test, average='binary')\n",
    "print(\"Accuracy of my model on test set: \", f1_score(y_true_test, y_pred_test))\n",
    "print(\"Precision: {}, Recall: {}, F1: {}\".format(precision_test, recall_test, f1_test))\n",
    "np.unique(y_pred_test, return_counts=True), np.unique(y_true_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './model/densenet101_{}_{}'.format(f1_train, f1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}